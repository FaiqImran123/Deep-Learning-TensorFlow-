{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graduate Admission Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "495         496        332          108                  5  4.5   4.0  9.02   \n",
       "496         497        337          117                  5  5.0   5.0  9.87   \n",
       "497         498        330          120                  5  4.5   5.0  9.56   \n",
       "498         499        312          103                  4  4.0   5.0  8.43   \n",
       "499         500        327          113                  4  4.5   4.5  9.04   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "495         1              0.87  \n",
       "496         1              0.96  \n",
       "497         1              0.93  \n",
       "498         0              0.73  \n",
       "499         0              0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv(\"Admission_Predict_Ver1.1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Serial No.\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "495        332          108                  5  4.5   4.0  9.02         1   \n",
       "496        337          117                  5  5.0   5.0  9.87         1   \n",
       "497        330          120                  5  4.5   5.0  9.56         1   \n",
       "498        312          103                  4  4.0   5.0  8.43         0   \n",
       "499        327          113                  4  4.5   4.5  9.04         0   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "495              0.87  \n",
       "496              0.96  \n",
       "497              0.93  \n",
       "498              0.73  \n",
       "499              0.84  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:, 0:-1]\n",
    "y =df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test =train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler =MinMaxScaler()\n",
    "X_train_scaled =scaler.fit_transform(X_train)\n",
    "X_test =scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faiqi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(7, activation=\"relu\", input_dim =7))\n",
    "model.add(Dense(7, activation=\"relu\", input_dim =7))\n",
    "# in case of regreesion Linear is used here output is probalistic so sigmoid is used\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0864 - val_loss: 0.0947\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0743 - val_loss: 0.0849\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0736 - val_loss: 0.0768\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0628 - val_loss: 0.0699\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0625 - val_loss: 0.0636\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0579 - val_loss: 0.0581\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0491 - val_loss: 0.0532\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0494 - val_loss: 0.0487\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0430 - val_loss: 0.0444\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0379 - val_loss: 0.0401\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0344 - val_loss: 0.0361\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0368 - val_loss: 0.0320\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0286 - val_loss: 0.0284\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0255 - val_loss: 0.0251\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0227 - val_loss: 0.0220\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0219 - val_loss: 0.0194\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0192 - val_loss: 0.0171\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0173 - val_loss: 0.0154\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0174 - val_loss: 0.0139\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0159 - val_loss: 0.0127\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0151 - val_loss: 0.0118\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0144 - val_loss: 0.0112\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0125 - val_loss: 0.0106\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0124 - val_loss: 0.0099\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0133 - val_loss: 0.0096\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0117 - val_loss: 0.0093\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0114 - val_loss: 0.0090\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0117 - val_loss: 0.0088\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0113 - val_loss: 0.0086\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0104 - val_loss: 0.0083\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0105 - val_loss: 0.0081\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0095 - val_loss: 0.0077\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0100 - val_loss: 0.0075\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - val_loss: 0.0072\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0089 - val_loss: 0.0070\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0086 - val_loss: 0.0069\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0105 - val_loss: 0.0068\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0077 - val_loss: 0.0066\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0088 - val_loss: 0.0064\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0069 - val_loss: 0.0063\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0082 - val_loss: 0.0063\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0086 - val_loss: 0.0062\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0069 - val_loss: 0.0061\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0076 - val_loss: 0.0060\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0069 - val_loss: 0.0059\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0081 - val_loss: 0.0059\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0071 - val_loss: 0.0058\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0074 - val_loss: 0.0058\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0070 - val_loss: 0.0057\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0067 - val_loss: 0.0055\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0066 - val_loss: 0.0053\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0061 - val_loss: 0.0049\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0058 - val_loss: 0.0049\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0061 - val_loss: 0.0050\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0059 - val_loss: 0.0048\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0060 - val_loss: 0.0048\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0056 - val_loss: 0.0046\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - val_loss: 0.0045\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0059 - val_loss: 0.0045\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0056 - val_loss: 0.0043\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0049 - val_loss: 0.0041\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0051 - val_loss: 0.0042\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0046 - val_loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "history =model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001237361F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001237361F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902694820009173"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.08613540232181549,\n",
       "  0.07746447622776031,\n",
       "  0.07044041901826859,\n",
       "  0.06405656039714813,\n",
       "  0.05850899964570999,\n",
       "  0.053249556571245193,\n",
       "  0.048795487731695175,\n",
       "  0.045046258717775345,\n",
       "  0.04149269685149193,\n",
       "  0.0380844846367836,\n",
       "  0.03466631472110748,\n",
       "  0.03163781017065048,\n",
       "  0.02843952737748623,\n",
       "  0.025587063282728195,\n",
       "  0.023164432495832443,\n",
       "  0.021039973944425583,\n",
       "  0.019210834056138992,\n",
       "  0.017644528299570084,\n",
       "  0.016547052189707756,\n",
       "  0.015599180944263935,\n",
       "  0.014934437349438667,\n",
       "  0.014366540126502514,\n",
       "  0.01400403119623661,\n",
       "  0.013680718839168549,\n",
       "  0.01338453870266676,\n",
       "  0.01308481115847826,\n",
       "  0.012759700417518616,\n",
       "  0.01237948052585125,\n",
       "  0.011958752758800983,\n",
       "  0.011481150984764099,\n",
       "  0.011065665632486343,\n",
       "  0.01069218385964632,\n",
       "  0.01031603291630745,\n",
       "  0.009940816089510918,\n",
       "  0.009600847028195858,\n",
       "  0.009285883978009224,\n",
       "  0.008973811753094196,\n",
       "  0.008721919730305672,\n",
       "  0.008446313440799713,\n",
       "  0.008210759609937668,\n",
       "  0.008001282811164856,\n",
       "  0.007813842967152596,\n",
       "  0.0076410770416259766,\n",
       "  0.0074777668341994286,\n",
       "  0.007332745939493179,\n",
       "  0.007206113077700138,\n",
       "  0.007123345043510199,\n",
       "  0.006986070424318314,\n",
       "  0.0068923779763281345,\n",
       "  0.006793152540922165,\n",
       "  0.006716914474964142,\n",
       "  0.006651917938143015,\n",
       "  0.006582159548997879,\n",
       "  0.006509917788207531,\n",
       "  0.006456238683313131,\n",
       "  0.006391962058842182,\n",
       "  0.006326085422188044,\n",
       "  0.0062766605988144875,\n",
       "  0.006214142311364412,\n",
       "  0.006169064901769161,\n",
       "  0.006111468188464642,\n",
       "  0.006060142535716295,\n",
       "  0.006016607396304607,\n",
       "  0.005950408522039652,\n",
       "  0.005902083124965429,\n",
       "  0.0058405534364283085,\n",
       "  0.00582471676170826,\n",
       "  0.005738506093621254,\n",
       "  0.00568573921918869,\n",
       "  0.005640971474349499,\n",
       "  0.0055884066969156265,\n",
       "  0.005554329603910446,\n",
       "  0.005482710897922516,\n",
       "  0.005440745502710342,\n",
       "  0.005361017771065235,\n",
       "  0.0053159152157604694,\n",
       "  0.005258435383439064,\n",
       "  0.005199058447033167,\n",
       "  0.005129252560436726,\n",
       "  0.005100478418171406,\n",
       "  0.005050656385719776,\n",
       "  0.0050146253779530525,\n",
       "  0.004965408705174923,\n",
       "  0.004915837664157152,\n",
       "  0.004872205201536417,\n",
       "  0.004837344400584698,\n",
       "  0.004788561724126339,\n",
       "  0.004752661567181349,\n",
       "  0.004721132107079029,\n",
       "  0.00468585966154933,\n",
       "  0.004652957431972027,\n",
       "  0.004631515592336655,\n",
       "  0.00459993164986372,\n",
       "  0.004557370208203793,\n",
       "  0.004540117923170328,\n",
       "  0.004503929056227207,\n",
       "  0.004493287298828363,\n",
       "  0.004444921854883432,\n",
       "  0.004428097512573004,\n",
       "  0.004412752576172352],\n",
       " 'val_loss': [0.09465993940830231,\n",
       "  0.0849192813038826,\n",
       "  0.0767894834280014,\n",
       "  0.06992167234420776,\n",
       "  0.06363241374492645,\n",
       "  0.05805611610412598,\n",
       "  0.053202033042907715,\n",
       "  0.04872319847345352,\n",
       "  0.04435940459370613,\n",
       "  0.04011286050081253,\n",
       "  0.03606965020298958,\n",
       "  0.03202691301703453,\n",
       "  0.028376296162605286,\n",
       "  0.025068184360861778,\n",
       "  0.02202543616294861,\n",
       "  0.019352922216057777,\n",
       "  0.017125630751252174,\n",
       "  0.015380236320197582,\n",
       "  0.013874022290110588,\n",
       "  0.012730796821415424,\n",
       "  0.011817125603556633,\n",
       "  0.011179090477526188,\n",
       "  0.010624878108501434,\n",
       "  0.010201334953308105,\n",
       "  0.009850201196968555,\n",
       "  0.00957854650914669,\n",
       "  0.009329861029982567,\n",
       "  0.00903228484094143,\n",
       "  0.008848758414387703,\n",
       "  0.008554073981940746,\n",
       "  0.008347352035343647,\n",
       "  0.008104586973786354,\n",
       "  0.007728388998657465,\n",
       "  0.0074845352210104465,\n",
       "  0.007224322762340307,\n",
       "  0.0069889710284769535,\n",
       "  0.006864356342703104,\n",
       "  0.006821315735578537,\n",
       "  0.006559588015079498,\n",
       "  0.00644698366522789,\n",
       "  0.006293821148574352,\n",
       "  0.006263266317546368,\n",
       "  0.006151456385850906,\n",
       "  0.00607298593968153,\n",
       "  0.005978032015264034,\n",
       "  0.005879526492208242,\n",
       "  0.005949496291577816,\n",
       "  0.005789394490420818,\n",
       "  0.005770867690443993,\n",
       "  0.005719692911952734,\n",
       "  0.005679730791598558,\n",
       "  0.005572388879954815,\n",
       "  0.0056715672835707664,\n",
       "  0.005529363639652729,\n",
       "  0.0056135267950594425,\n",
       "  0.005415610037744045,\n",
       "  0.00541312200948596,\n",
       "  0.005508524365723133,\n",
       "  0.005434555467218161,\n",
       "  0.005251196213066578,\n",
       "  0.00524481013417244,\n",
       "  0.0052362727001309395,\n",
       "  0.005309573374688625,\n",
       "  0.005141614004969597,\n",
       "  0.005062205716967583,\n",
       "  0.005086695309728384,\n",
       "  0.005186166614294052,\n",
       "  0.005017015151679516,\n",
       "  0.004939197096973658,\n",
       "  0.004893275443464518,\n",
       "  0.004934715107083321,\n",
       "  0.004987643100321293,\n",
       "  0.004843996372073889,\n",
       "  0.004738409072160721,\n",
       "  0.004770336672663689,\n",
       "  0.004762447439134121,\n",
       "  0.00463029183447361,\n",
       "  0.004613784141838551,\n",
       "  0.004528731107711792,\n",
       "  0.004583489149808884,\n",
       "  0.00453973701223731,\n",
       "  0.004310340620577335,\n",
       "  0.004382596351206303,\n",
       "  0.004402408841997385,\n",
       "  0.004286099225282669,\n",
       "  0.004277314990758896,\n",
       "  0.004206160549074411,\n",
       "  0.004218054004013538,\n",
       "  0.004200874827802181,\n",
       "  0.004123723600059748,\n",
       "  0.004113899543881416,\n",
       "  0.004169217310845852,\n",
       "  0.004068833310157061,\n",
       "  0.004080855753272772,\n",
       "  0.004090537317097187,\n",
       "  0.0040245940908789635,\n",
       "  0.003988170996308327,\n",
       "  0.0040302821435034275,\n",
       "  0.004079578444361687,\n",
       "  0.003981915768235922]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x123736827b0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
